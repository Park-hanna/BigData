{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcca355-2c41-48c6-9e83-a990c153d3f9",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867bb261-ffe7-4338-9c22-5b9fa30de6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entity recognition task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63db75fd-7bf9-4b6a-99ec-beab63ac8914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPyTorch의 TensorDataset은 기본적으로 x[index], y[index]를 제공합니다.\\n그 외에 추가로 제공하고 싶은게 있으면 아래와 같이 커스텀이 가능합니다.\\n여기서는 입력되는 문장의 길이를 제공 받아야해서 아래와 같이 커스텀을 하였습니다.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from torchcrf import CRF\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.dataset import Preprocessing\n",
    "from src.model import EpochLogger, MakeEmbed, save\n",
    "\n",
    "'''\n",
    "PyTorch의 TensorDataset은 기본적으로 x[index], y[index]를 제공합니다.\n",
    "그 외에 추가로 제공하고 싶은게 있으면 아래와 같이 커스텀이 가능합니다.\n",
    "여기서는 입력되는 문장의 길이를 제공 받아야해서 아래와 같이 커스텀을 하였습니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3478bb6b-bbcb-4d9f-94a3-e02c8f5d6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset(data.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor, lengths):\n",
    "        super(EntityDataset, self).__init__()\n",
    "        \n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        self.lengths = lengths\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index], self.lengths[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5aa4cc9-1305-4c12-a3b8-5e7e0ed202a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.entity_label_dir = './chatbot_data/dataset/entity_label.json'\n",
    "        self.entity_data_dir = './chatbot_data/dataset/entity_data.csv'\n",
    "        \n",
    "        self.entity_label = self.load_entity_label()\n",
    "        self.prep = Preprocessing()\n",
    "        \n",
    "    def load_entity_label(self):\n",
    "        f = open(self.entity_label_dir, encoding = 'UTF-8')\n",
    "        entity_label = json.loads(f.read())\n",
    "        self.entitys = list(entity_label.keys())\n",
    "        return entity_label\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return sentence.split()\n",
    "    \n",
    "    def tokenize_dataset(self, dataset):\n",
    "        token_dataset = []\n",
    "        for data in dataset:\n",
    "            token_dataset.append(self.tokenize(data))\n",
    "        return token_dataset\n",
    "    \n",
    "    def make_entity_dataset(self, embed):\n",
    "        entity_dataset = pd.read_csv(self.entity_data_dir)\n",
    "        entity_querys = self.tokenize_dataset(entity_dataset['question'].tolist())\n",
    "        labels = []\n",
    "        for label in entity_dataset['label'].to_list():\n",
    "            temp = []\n",
    "            for entity in label.split():\n",
    "                temp.append(self.entity_label[entity])\n",
    "            labels.append(temp)\n",
    "        dataset = list(zip(entity_querys, labels))\n",
    "        entity_train_dataset, entity_test_dataset = self.word2idx_dataset(dataset, embed)\n",
    "        return entity_train_dataset, entity_test_dataset\n",
    "    \n",
    "    def word2idx_dataset(self, dataset, embed, train_ratio=0.8):\n",
    "        embed_dataset = []\n",
    "        question_list, label_list, lengths = [], [], []\n",
    "        flag = True\n",
    "        random.shuffle(dataset)\n",
    "        for query, label in dataset:\n",
    "            q_vec = embed.query2idx(query)\n",
    "            lengths.append(len(q_vec))\n",
    "            \n",
    "            q_vec = self.prep.pad_idx_sequencing(q_vec)\n",
    "            \n",
    "            question_list.append(torch.tensor([q_vec]))\n",
    "            \n",
    "            label = self.prep.pad_idx_sequencing(label)\n",
    "            label_list.append(label)\n",
    "            flag = False\n",
    "            \n",
    "        x = torch.cat(question_list)\n",
    "        y = torch.tensor(label_list)\n",
    "        \n",
    "        x_len = x.size()[0]\n",
    "        y_len = y.size()[0]\n",
    "        if(x_len == y_len):\n",
    "            train_size = int(x_len * train_ratio)\n",
    "            \n",
    "            train_x = x[:train_size]\n",
    "            train_y = y[:train_size]\n",
    "            \n",
    "            test_x = x[train_size+1:]\n",
    "            test_y = y[train_size+1:]\n",
    "            \n",
    "            train_length = lengths[:train_size]\n",
    "            test_length = lengths[:train_size]\n",
    "            \n",
    "            train_dataset = EntityDataset(train_x, train_y, train_length)\n",
    "            test_dataset = EntityDataset(test_x, test_y, test_length)\n",
    "            \n",
    "            return train_dataset, test_dataset\n",
    "        \n",
    "        else:\n",
    "            print('ERROR x!=y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05869a54-9fbf-4280-89da-bf4a3f8fa30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MakeDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d29add1b-1135-4843-9275-21e08e7306e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-DATE': 1,\n",
       " 'B-LOCATION': 2,\n",
       " 'B-PLACE': 3,\n",
       " 'B-RESTAURANT': 4,\n",
       " 'E-DATE': 5,\n",
       " 'E-LOCATION': 6,\n",
       " 'E-PLACE': 7,\n",
       " 'E-RESTAURANT': 8,\n",
       " 'I-DATE': 9,\n",
       " 'I-RESTAURANT': 10,\n",
       " 'S-DATE': 11,\n",
       " 'S-LOCATION': 12,\n",
       " 'S-PLACE': 13,\n",
       " 'S-RESTAURANT': 14,\n",
       " '<START_TAG>': 15,\n",
       " '<STOP_TAG>': 16}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Inside, Out, Begin, End, Single\n",
    "IO : TAG 라면 I , 아니면 O로 TAG\n",
    "BIO : TAG의 길이가 2이상이면 첫 번째 단어는 B를 붙임 그 뒤의 단어들은 I를 붙임\n",
    "BIOES : BIO에서 단어의 길이가 3이상인 단어는 마지막 단어에 E를 붙임, 그리고 단어의 길이가 1이라면 S를 붙임\n",
    "S : 단독\n",
    "B : 복합의 시작 -> 단독 사용 불가\n",
    "I : 복합의 중간 -> 단독 사용 불가\n",
    "E : 복힙의 끝 -> 단독 사용 불가\n",
    "O : 의미 없음\n",
    "'''\n",
    "dataset.entity_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22dcf25d-9455-4a33-97f4-b783724cd6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>야 먼지 알려주겠니</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아니 먼지 정보 알려주세요</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그 때 미세먼지 어떨까</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그 때 먼지 좋으려나</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미세먼지 어떨 것 같은데</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         question    label\n",
       "0      야 먼지 알려주겠니    O O O\n",
       "1  아니 먼지 정보 알려주세요  O O O O\n",
       "2    그 때 미세먼지 어떨까  O O O O\n",
       "3     그 때 먼지 좋으려나  O O O O\n",
       "4   미세먼지 어떨 것 같은데  O O O O"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dataset = pd.read_csv(dataset.entity_data_dir)\n",
    "\n",
    "entity_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf430a8-1c04-4074-ad6c-e2e246929b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE B-LOCATION E-LOCATION O O</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O B-LOCATION E-LOCATION O O O O</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O O</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O O O</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT O S-LOCATION O O</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT O S-RESTAURANT O</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT S-LOCATION O</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT S-LOCATION O O O</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT S-RESTAURANT O O</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question\n",
       "label                                                  \n",
       "B-DATE E-DATE B-LOCATION E-LOCATION O O               4\n",
       "B-DATE E-DATE O                                       6\n",
       "B-DATE E-DATE O B-LOCATION E-LOCATION O O O O         1\n",
       "B-DATE E-DATE O O                                    42\n",
       "B-DATE E-DATE O O O                                  31\n",
       "...                                                 ...\n",
       "S-RESTAURANT O S-LOCATION O O                         4\n",
       "S-RESTAURANT O S-RESTAURANT O                         3\n",
       "S-RESTAURANT S-LOCATION O                             1\n",
       "S-RESTAURANT S-LOCATION O O O                         1\n",
       "S-RESTAURANT S-RESTAURANT O O                         2\n",
       "\n",
       "[354 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dataset.groupby(['label']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e14b3-38b4-4784-ad95-b9364c93d1c3",
   "metadata": {},
   "source": [
    "# Bidirctional LSTM - CRF Models for sequence Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10025e55-144b-4bdb-a8d5-476b927dafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, w2v, tag_to_ix, hidden_dim, batch_size):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = w2v.size()[1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = w2v.size()[0]\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(teg_to_ix)\n",
    "        self.batch_size = batch_size\n",
    "        self.START_TAG = \"<START_TAG>\"\n",
    "        self.STOP_TAG = \"<STOP_TAG>\"\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(self.vocab_size+2, self.embedding_dim)\n",
    "        self.word_embeds.weight[2:].data.copy_(w2v)\n",
    "        # self.word_embeds.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM Parameter Define\n",
    "        # bidirectional : 2 way LSTM\n",
    "        # num_layers : layer count\n",
    "        # batch_first : pytorch LSTM input default value : (Length, batch, Hidden) line  then change the (batch, Length, Hidden)\n",
    "        # nn.LSTM(input_size, hidden_size, batch_first, num_layers)\n",
    "        # hidden_size = hidden_dim // reason the 2 because bidirectional = True\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim // 2 , batch_first = True, num_layers = 1, bidirectional = True )\n",
    "        \n",
    "        # LSTM output : Tag position \n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        # ouputing : rule train CRF setting\n",
    "        self.crf = CRF(self.tagset_size, batch_first = True)\n",
    "        \n",
    "    def init_hidden(self): #(h,c)\n",
    "        return (torch.randn(2, self.batch_size, self.hidden_dim // 2),\n",
    "                torch.randn(2, self.batch_size, self.hidden_dim // 2))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        # Bi-LSTM : output score get\n",
    "        self.batch_size = sentence.size()[0]\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        #(2, 128, 128), (2, 128, 128)\n",
    "        embeds = self.word_embeds(sentence)\n",
    "        \n",
    "        #(128, 20, 300)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        #(128, 20, 256), ((2, 128, 128), (2, 128, 128))\n",
    "        lstm_feats = self.hidden2tag(lstm_out) # (batch, length, tagset_size)\n",
    "        \n",
    "        #(128, 20, 17)\n",
    "        return lstm_feats\n",
    "    \n",
    "    def decode(self, logits, mask):\n",
    "        \"\"\"\n",
    "        Viterbi Decoding의 구현체입니다.\n",
    "        CRF 레이어의 출력을 prediction으로 변형합니다.\n",
    "        :param logits: 모델의 출력 (로짓)\n",
    "        :param mask: 마스킹 벡터\n",
    "        :return: 모델의 예측 (prediction)\n",
    "        \n",
    "        각 단어의 자리마다\n",
    "          word 1의 태그 확률        |  word2의 태그 확률\n",
    "         'O': 확률0,              | 'O': 확률A,\n",
    "         'B-DATE': 확률1,         | 'B-DATE': 확률B\n",
    "         'B-LOCATION': 확률2,     | 'B-LOCATION': 확률C,\n",
    "         'B-PLACE': 확률3,        | 'B-PLACE': 확률D,  \n",
    "         'B-RESTAURANT': 확률4,   | 'B-RESTAURANT': 확률E,  \n",
    "         'E-DATE': 확률5,         | 'E-DATE': 확률F,   \n",
    "         'E-LOCATION': 확률6,     | 'E-LOCATION': 확률G, \n",
    "         'E-PLACE': 확률7,        | 'E-PLACE': 확률H,  \n",
    "         'E-RESTAURANT': 확률8,   | 'E-RESTAURANT': 확률I, \n",
    "         'I-DATE': 확률9,         | 'I-DATE': 확률J,    \n",
    "         'I-RESTAURANT': 확률10,  | 'I-RESTAURANT': 확률K,\n",
    "         'S-DATE': 확률11,        | 'S-DATE': 확률L,      \n",
    "         'S-LOCATION': 확률12,    | 'S-LOCATION': 확률M,\n",
    "         'S-PLACE': 확률13,       | 'S-PLACE': 확률N,  \n",
    "         'S-RESTAURANT': 확률14,  | 'S-RESTAURANT': 확률O,\n",
    "         '<START_TAG>': 확률15,   | '<START_TAG>': 확률P, \n",
    "         '<STOP_TAG>': 확률15,    | '<STOP_TAG>': 확률Q,\n",
    "         \n",
    "         각각의 높은 확률을 뽑는 것은 보통의 딥러닝 방식으로 B단독이나 I단독, E단독같은 문제를 야기할 수 있습니다.\n",
    "         태그들의 확률 값을 받아서 \n",
    "         CRF는 태그들의 의존성을 학습할수 있어서 태그 시퀀스의 확률이 가장 높은 확률을 가지는 예측 시퀀스를 선택한다.\n",
    "         그래서 B단독이나 I단독, E단독과 같은 문제를 없애줍니다.\n",
    "         예를 들어 B-DATE, O 와 같은걸 출력하지 않습니다. (CRF는 S-DATE, O 라고 출력합니다.)\n",
    "        \"\"\"\n",
    "        return self.crf.decode(logits, mask)\n",
    "    \n",
    "    def compute_loss(self, label, logits, mask):\n",
    "        \"\"\"\n",
    "        학습을 위한 total loss 계산\n",
    "        :param label : label\n",
    "        :param logits : logits\n",
    "        :param mask : mask vector\n",
    "        :return : total loss\n",
    "        \"\"\"\n",
    "        \n",
    "        log_likelihood = self.crf(logits, label, mask = mask, reduction='mean')\n",
    "        return - log_likelihood  # Negative log likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bae5f1a-664d-4795-9415-b48f47aaa568",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'questions_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m embed \u001b[38;5;241m=\u001b[39m MakeEmbed()\n\u001b[0;32m      2\u001b[0m embed\u001b[38;5;241m.\u001b[39mload_word2vec()\n\u001b[1;32m----> 4\u001b[0m entity_train_dataset, entity_test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_entity_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(entity_train_dataset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(entity_test_dataset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[5], line 35\u001b[0m, in \u001b[0;36mMakeDataset.make_entity_dataset\u001b[1;34m(self, embed)\u001b[0m\n\u001b[0;32m     33\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(temp)\n\u001b[0;32m     34\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(entity_querys, labels))\n\u001b[1;32m---> 35\u001b[0m entity_train_dataset, entity_test_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword2idx_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m entity_train_dataset, entity_test_dataset\n",
      "Cell \u001b[1;32mIn[5], line 49\u001b[0m, in \u001b[0;36mMakeDataset.word2idx_dataset\u001b[1;34m(self, dataset, embed, train_ratio)\u001b[0m\n\u001b[0;32m     45\u001b[0m lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(q_vec))\n\u001b[0;32m     47\u001b[0m q_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep\u001b[38;5;241m.\u001b[39mpad_idx_sequencing(q_vec)\n\u001b[1;32m---> 49\u001b[0m \u001b[43mquestions_list\u001b[49m\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor([q_vec]))\n\u001b[0;32m     51\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep\u001b[38;5;241m.\u001b[39mpad_idx_sequencing(label)\n\u001b[0;32m     52\u001b[0m label_list\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'questions_list' is not defined"
     ]
    }
   ],
   "source": [
    "embed = MakeEmbed()\n",
    "embed.load_word2vec()\n",
    "\n",
    "entity_train_dataset, entity_test_dataset = dataset.make_entity_dataset(embed)\n",
    "\n",
    "train_dataloader = DataLoader(entity_train_dataset, batch_size = 128, shuffle = True)\n",
    "\n",
    "test_dataloader = DataLoader(entity_test_dataset, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc7b71-9fc3-4688-a256-6653c1c525a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
