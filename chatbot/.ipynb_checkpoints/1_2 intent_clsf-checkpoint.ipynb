{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f52a00-2137-4514-a775-adfc387fa687",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9fa103-edb5-4177-a497-c6a18bc6ef66",
   "metadata": {},
   "source": [
    "## Intent classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d219bcaa-ed73-49ae-be68-aad896bc8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "from src.dataset import Preprocessing\n",
    "from src.model import EpochLogger, MakeEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffeb5da-22e3-467e-bc91-a44b3b746b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.intent_label_dir = './chatbot_data/dataset/intent_label.json'\n",
    "        self.intent_data_dir = './chatbot_data/dataset/intent_data.csv'\n",
    "        \n",
    "        self.intent_label = self.load_intent_label()\n",
    "        self.prep = Preprocessing()\n",
    "        \n",
    "    def load_intent_label(self):\n",
    "        '''미리 만들어 둔 예측해야할 intent label load'''\n",
    "        f = open(self.intent_label_dir, encoding='UTF-8')\n",
    "        intent_label = json.loads(f.read())\n",
    "        self.intents = list(intent_label.keys())\n",
    "        return intent_label\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        '''띄어쓰기 단위로 tokenize 적용'''\n",
    "        return sentence.split()\n",
    "\n",
    "    def tokenize_dataset(self, dataset):\n",
    "        '''Dataset 에 tokenize 적용'''\n",
    "        token_dataset = []\n",
    "        for data in dataset:\n",
    "            token_dataset.append(self.tokenize(data))\n",
    "        return token_dataset\n",
    "\n",
    "    def make_intent_dataset(self, embed):\n",
    "        '''intent 분류를 위한 Dataset 생성'''\n",
    "        intent_dataset = pd.read_csv(self.intent_data_dir) # data loding\n",
    "\n",
    "        labels = [self.intent_label[label] for label in intent_dataset['label'].to_list()] # label\n",
    "\n",
    "        intent_querys = self.tokenize_dataset(intent_dataset['question'].tolist()) # user ignition tokenize\n",
    "\n",
    "        dataset = list(zip(intent_querys, labels)) # (user ignition , intent) shape \n",
    "        intent_train_dataset, intent_test_dataset = self.word2idx_dataset(dataset, embed) # word2index\n",
    "        return intent_train_dataset, intent_test_dataset\n",
    "\n",
    "    def word2idx_dataset(self, dataset, embed, train_ratio=0.8):\n",
    "        embed_dataset = []\n",
    "        question_list, label_list = [], []\n",
    "        flag = True\n",
    "        random.shuffle(dataset) # split a training and validation dataset \n",
    "        for query, label in dataset:\n",
    "            q_vec = embed.query2idx(query) # user ignintion index\n",
    "            q_vec = self.prep.pad_idx_sequencing(q_vec) # user ignition maximum lendgth padding\n",
    "\n",
    "            question_list.append(torch.tensor([q_vec]))\n",
    "            label_list.append(torch.tensor([label]))\n",
    "\n",
    "        x = torch.cat(question_list)\n",
    "        y = torch.cat(label_list)\n",
    "\n",
    "        #split train, validation data\n",
    "        x_len = x.size()[0]\n",
    "        y_len = y.size()[0]\n",
    "        if(x_len == y_len):\n",
    "            train_size = int(x_len * train_ratio)\n",
    "\n",
    "            train_x = x[:train_size]\n",
    "            train_y = y[:train_size]\n",
    "\n",
    "            test_x = x[train_size +1 :]\n",
    "            test_y = y[train_size +1 :]\n",
    "\n",
    "            # Covering the Tensor Dataset\n",
    "            '''\n",
    "             PyTorch의 TensorDataset은 tensor를 감싸는 Dataset입니다.\n",
    "             인덱싱 방식과 길이를 정의함으로써 이것은 tensor의 첫 번째 차원을 따라 반복, 인덱스, 슬라이스를 위한 방법을 제공합니다.\n",
    "             훈련할 때 동일한 라인에서 독립 변수와 종속 변수에 쉽게 접근할 수 있습니다.\n",
    "            '''\n",
    "            train_dataset = TensorDataset(train_x, train_y)\n",
    "            test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "            return train_dataset, test_dataset\n",
    "\n",
    "        else:\n",
    "            print('ERROR x!=y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3fd90a-32c9-4636-8761-141124d84b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MakeDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb73f72-55fe-4ee7-98a7-7f05f46eeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dataset = pd.read_csv(dataset.intent_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8776b49b-2752-412d-b0f6-6dbd4554bd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>야 먼지 알려주겠니</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아니 먼지 정보 알려주세요</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그 때 미세먼지 어떨까</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그 때 먼지 좋으려나</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미세먼지 어떨 것 같은데</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         question label\n",
       "0      야 먼지 알려주겠니  dust\n",
       "1  아니 먼지 정보 알려주세요  dust\n",
       "2    그 때 미세먼지 어떨까  dust\n",
       "3     그 때 먼지 좋으려나  dust\n",
       "4   미세먼지 어떨 것 같은데  dust"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e90db5a6-4c2e-424e-a7fa-937fc502d60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dust</th>\n",
       "      <td>4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restaurant</th>\n",
       "      <td>4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather</th>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question\n",
       "label               \n",
       "dust            4997\n",
       "restaurant      4997\n",
       "travel          4999\n",
       "weather         4999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_dataset.groupby(['label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97997ba6-d9d9-4e9f-9c02-fbd0d721429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = MakeEmbed()\n",
    "embed.load_word2vec()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "intent_train_dataset, intent_test_dataset = dataset.make_intent_dataset(embed)\n",
    "\n",
    "# 1 epoch per batch size x, y\n",
    "train_dataloader = DataLoader(intent_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(intent_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af394770-3f03-414c-9bc3-52f3a73d6a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[147, 244,  31,  ...,   0,   0,   0],\n",
       "         [  8, 579,   4,  ...,   0,   0,   0],\n",
       "         [364, 249,  17,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [ 33,  30, 187,  ...,   0,   0,   0],\n",
       "         [  8, 131, 133,  ...,   0,   0,   0],\n",
       "         [304, 223,   3,  ...,   0,   0,   0]]),\n",
       " tensor([1, 3, 1,  ..., 3, 3, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_train_dataset.tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770bd5b-08d3-46f0-9d1c-8ecf1a2d111a",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08da1835-cd95-41cc-9ef0-84a05dce061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class textCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, w2v, dim, kernels, dropout, num_class):\n",
    "        super(textCNN, self).__init__()\n",
    "        #Apply pretrained embedding with Word2vec\n",
    "        vocab_size = w2v.size()[0]\n",
    "        emb_dim = w2v.size()[1]\n",
    "        self.embed = nn.Embedding(vocab_size+2, emb_dim)\n",
    "        self.embed.weight[2:].data.copy_(w2v)\n",
    "        # self.embed.weight.requires_grad = False\n",
    "        # embeding layer train y or n\n",
    "        \n",
    "        # window size : per each conv layer -> nn.ModuleList save\n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim, (w, emb_dim)) for w in kernels])\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # FC layer\n",
    "        self.fc = nn.Linear(len(kernels)*dim, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb_x = self.embed(x)\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "        \n",
    "        con_x = [conv(emb_x) for conv in self.convs] # each size per result -> list save\n",
    "        #[(out_channels, conv result length)...]\n",
    "        \n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x] # each size per max_pool result -> save\n",
    "        #[(256, 1)]\n",
    "        \n",
    "        fc_x = torch.cat(pool_x, dim=1) #concat -> fc layer input shape make\n",
    "        # (768, 1)\n",
    "        \n",
    "        fc_x = fc_x.squeeze(-1) # conv select\n",
    "        #(768)\n",
    "        \n",
    "        fc_x = self.dropout(fc_x)\n",
    "        logit = self.fc(fc_x)\n",
    "        return logit\n",
    "    \n",
    "    # model weight save code\n",
    "    def save(model, save_dir, save_prefix, epoch):\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_prefix = os.path.join(save_dir, save_prefix)\n",
    "        save_path = '{}_steps_{}.pt'.format(save_prefix, epoch)\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "082ea262-aff9-418e-be08-a3319492609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = embed.word2vec.wv.vectors # word2vec weight\n",
    "weights = torch.FloatTensor(weights)\n",
    "\n",
    "num_class = len(dataset.intent_label) \n",
    "model = textCNN(weights, 256, [3,4,5], 0.5, num_class)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf4e25fe-0ce5-42a8-984f-b787ace25b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textCNN(\n",
       "  (embed): Embedding(1481, 300)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 256, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c0fc91-6680-4156-b7d0-22069a56c5d6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e59cfdd-b6e8-426d-9527-82bbb2055f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████████| 125/125 [00:17<00:00,  7.32batch/s, accuracy=99.17355, loss=1.39]\n",
      "Epoch 0: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 24.36batch/s, accuracy=99.4, loss=0]\n",
      "Epoch 1: 100%|██████████████████████████████████████| 125/125 [00:15<00:00,  7.89batch/s, accuracy=100.0, loss=1.97e-9]\n",
      "Epoch 1: 100%|███████████████████████████████████████████| 32/32 [00:01<00:00, 24.57batch/s, accuracy=99.2, loss=0.666]\n",
      "Epoch 2: 100%|█████████████████████████████████████| 125/125 [00:15<00:00,  7.96batch/s, accuracy=99.17355, loss=0.648]\n",
      "Epoch 2: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 24.43batch/s, accuracy=99.3, loss=0]\n",
      "Epoch 3: 100%|████████████████████████████████████████████| 125/125 [00:15<00:00,  7.85batch/s, accuracy=100.0, loss=0]\n",
      "Epoch 3: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 24.29batch/s, accuracy=99.3, loss=0]\n",
      "Epoch 4: 100%|██████████████████████████████████████| 125/125 [00:15<00:00,  8.08batch/s, accuracy=100.0, loss=1.97e-9]\n",
      "Epoch 4: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 24.68batch/s, accuracy=99.3, loss=0]\n",
      "Epoch 5: 100%|████████████████████████████████████████████| 125/125 [00:15<00:00,  8.26batch/s, accuracy=100.0, loss=0]\n",
      "Epoch 5: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 24.31batch/s, accuracy=99.3, loss=0]\n",
      "Epoch 6: 100%|████████████████████████████████████████████| 125/125 [00:14<00:00,  8.40batch/s, accuracy=100.0, loss=0]\n",
      "Epoch 6: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 24.10batch/s, accuracy=99.4, loss=0]\n",
      "Epoch 7: 100%|████████████████████████████████████████████| 125/125 [00:14<00:00,  8.34batch/s, accuracy=100.0, loss=0]\n",
      "Epoch 7: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 23.59batch/s, accuracy=99.6, loss=0]\n",
      "Epoch 8: 100%|████████████████████████████████████████████| 125/125 [00:15<00:00,  8.30batch/s, accuracy=100.0, loss=0]\n",
      "Epoch 8: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 23.49batch/s, accuracy=99.3, loss=0]\n",
      "Epoch 9: 100%|████████████████████████████████████████████| 125/125 [00:15<00:00,  7.98batch/s, accuracy=100.0, loss=0]\n",
      "Epoch 9: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 22.72batch/s, accuracy=99.2, loss=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 11min 10s\n",
      "Wall time: 2min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch = 10\n",
    "prev_acc = 0\n",
    "save_dir = './chatbot_data/pretraining/1_intent_clsf_model/'\n",
    "save_prefix = 'intent_clsf'\n",
    "\n",
    "for i in range(epoch):\n",
    "    steps = 0\n",
    "    model.train() \n",
    "    #for data in train_dataloader:\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "            logit = model.forward(x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(logit, target) # loass function\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * corrects/x.size()[0]\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= accuracy.numpy())\n",
    "            \n",
    "    model.eval() # weight 업데이트 금지\n",
    "    steps = 0\n",
    "    accuarcy_list = []\n",
    "    #for data in test_dataloader:\n",
    "    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "\n",
    "            logit = model.forward(x)\n",
    "            loss = F.cross_entropy(logit, target)\n",
    "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * corrects/x.size()[0]\n",
    "            accuarcy_list.append(accuracy.tolist())\n",
    "            \n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= sum(accuarcy_list)/len(accuarcy_list))\n",
    "    \n",
    "    # epoch 당 검증 셋의 정확도를 계산하고 이전 정확도 보다 높으면 저장     \n",
    "    acc = sum(accuarcy_list)/len(accuarcy_list)\n",
    "    if(acc>prev_acc):\n",
    "        prev_acc = acc\n",
    "        textCNN.save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294adc76-6eb8-4589-963b-b63a0b6b512d",
   "metadata": {},
   "source": [
    "# Load & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b210a4f5-36d2-47cf-93cf-efcddba40d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textCNN(\n",
       "  (embed): Embedding(1481, 300)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 256, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./chatbot_data/pretraining/1_intent_clsf_model/intent_clsf_99.585_steps_7.pt'))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45a4c5ab-6275-4a61-8bae-78121abe9c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발화 : 제주도 오늘 날씨 알려줘\n",
      "의도 : weather\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 6.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = '제주도 오늘 날씨 알려줘'\n",
    "\n",
    "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
    "\n",
    "x = torch.tensor(x)\n",
    "f = model(x.unsqueeze(0))\n",
    "\n",
    "intent = dataset.intents[torch.argmax(f).tolist()]\n",
    "\n",
    "print('발화 : ' + q)\n",
    "print('의도 : ' + intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7fa3e-b72d-4cc0-8abf-7611eaaf02c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
