{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcca355-2c41-48c6-9e83-a990c153d3f9",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867bb261-ffe7-4338-9c22-5b9fa30de6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entity recognition task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63db75fd-7bf9-4b6a-99ec-beab63ac8914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPyTorch의 TensorDataset은 기본적으로 x[index], y[index]를 제공합니다.\\n그 외에 추가로 제공하고 싶은게 있으면 아래와 같이 커스텀이 가능합니다.\\n여기서는 입력되는 문장의 길이를 제공 받아야해서 아래와 같이 커스텀을 하였습니다.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from torchcrf import CRF\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.dataset import Preprocessing\n",
    "from src.model import EpochLogger, MakeEmbed, save\n",
    "\n",
    "'''\n",
    "PyTorch의 TensorDataset은 기본적으로 x[index], y[index]를 제공합니다.\n",
    "그 외에 추가로 제공하고 싶은게 있으면 아래와 같이 커스텀이 가능합니다.\n",
    "여기서는 입력되는 문장의 길이를 제공 받아야해서 아래와 같이 커스텀을 하였습니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3478bb6b-bbcb-4d9f-94a3-e02c8f5d6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset(data.Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor, lengths):\n",
    "        super(EntityDataset, self).__init__()\n",
    "        \n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        self.lengths = lengths\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index], self.lengths[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5aa4cc9-1305-4c12-a3b8-5e7e0ed202a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.entity_label_dir = './chatbot_data/dataset/entity_label.json'\n",
    "        self.entity_data_dir = './chatbot_data/dataset/entity_data.csv'\n",
    "        \n",
    "        self.entity_label = self.load_entity_label()\n",
    "        self.prep = Preprocessing()\n",
    "        \n",
    "    def load_entity_label(self):\n",
    "        f = open(self.entity_label_dir, encoding = 'UTF-8')\n",
    "        entity_label = json.loads(f.read())\n",
    "        self.entitys = list(entity_label.keys())\n",
    "        return entity_label\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return sentence.split()\n",
    "    \n",
    "    def tokenize_dataset(self, dataset):\n",
    "        token_dataset = []\n",
    "        for data in dataset:\n",
    "            token_dataset.append(self.tokenize(data))\n",
    "        return token_dataset\n",
    "    \n",
    "    def make_entity_dataset(self, embed):\n",
    "        entity_dataset = pd.read_csv(self.entity_data_dir)\n",
    "        entity_querys = self.tokenize_dataset(entity_dataset['question'].tolist())\n",
    "        labels = []\n",
    "        for label in entity_dataset['label'].to_list():\n",
    "            temp = []\n",
    "            for entity in label.split():\n",
    "                temp.append(self.entity_label[entity])\n",
    "            labels.append(temp)\n",
    "        dataset = list(zip(entity_querys, labels))\n",
    "        entity_train_dataset, entity_test_dataset = self.word2idx_dataset(dataset, embed)\n",
    "        return entity_train_dataset, entity_test_dataset\n",
    "    \n",
    "    def word2idx_dataset(self, dataset, embed, train_ratio=0.8):\n",
    "        embed_dataset = []\n",
    "        question_list, label_list, lengths = [], [], []\n",
    "        flag = True\n",
    "        random.shuffle(dataset)\n",
    "        for query, label in dataset:\n",
    "            q_vec = embed.query2idx(query)\n",
    "            lengths.append(len(q_vec))\n",
    "            \n",
    "            q_vec = self.prep.pad_idx_sequencing(q_vec)\n",
    "            \n",
    "            question_list.append(torch.tensor([q_vec]))\n",
    "            \n",
    "            label = self.prep.pad_idx_sequencing(label)\n",
    "            label_list.append(label)\n",
    "            flag = False\n",
    "            \n",
    "        x = torch.cat(question_list)\n",
    "        y = torch.tensor(label_list)\n",
    "        \n",
    "        x_len = x.size()[0]\n",
    "        y_len = y.size()[0]\n",
    "        if(x_len == y_len):\n",
    "            train_size = int(x_len * train_ratio)\n",
    "            \n",
    "            train_x = x[:train_size]\n",
    "            train_y = y[:train_size]\n",
    "            \n",
    "            test_x = x[train_size+1:]\n",
    "            test_y = y[train_size+1:]\n",
    "            \n",
    "            train_length = lengths[:train_size]\n",
    "            test_length = lengths[:train_size]\n",
    "            \n",
    "            train_dataset = EntityDataset(train_x, train_y, train_length)\n",
    "            test_dataset = EntityDataset(test_x, test_y, test_length)\n",
    "            \n",
    "            return train_dataset, test_dataset\n",
    "        \n",
    "        else:\n",
    "            print('ERROR x!=y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05869a54-9fbf-4280-89da-bf4a3f8fa30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MakeDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d29add1b-1135-4843-9275-21e08e7306e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-DATE': 1,\n",
       " 'B-LOCATION': 2,\n",
       " 'B-PLACE': 3,\n",
       " 'B-RESTAURANT': 4,\n",
       " 'E-DATE': 5,\n",
       " 'E-LOCATION': 6,\n",
       " 'E-PLACE': 7,\n",
       " 'E-RESTAURANT': 8,\n",
       " 'I-DATE': 9,\n",
       " 'I-RESTAURANT': 10,\n",
       " 'S-DATE': 11,\n",
       " 'S-LOCATION': 12,\n",
       " 'S-PLACE': 13,\n",
       " 'S-RESTAURANT': 14,\n",
       " '<START_TAG>': 15,\n",
       " '<STOP_TAG>': 16}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Inside, Out, Begin, End, Single\n",
    "IO : TAG 라면 I , 아니면 O로 TAG\n",
    "BIO : TAG의 길이가 2이상이면 첫 번째 단어는 B를 붙임 그 뒤의 단어들은 I를 붙임\n",
    "BIOES : BIO에서 단어의 길이가 3이상인 단어는 마지막 단어에 E를 붙임, 그리고 단어의 길이가 1이라면 S를 붙임\n",
    "S : 단독\n",
    "B : 복합의 시작 -> 단독 사용 불가\n",
    "I : 복합의 중간 -> 단독 사용 불가\n",
    "E : 복힙의 끝 -> 단독 사용 불가\n",
    "O : 의미 없음\n",
    "'''\n",
    "dataset.entity_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22dcf25d-9455-4a33-97f4-b783724cd6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>야 먼지 알려주겠니</td>\n",
       "      <td>O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아니 먼지 정보 알려주세요</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그 때 미세먼지 어떨까</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그 때 먼지 좋으려나</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미세먼지 어떨 것 같은데</td>\n",
       "      <td>O O O O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         question    label\n",
       "0      야 먼지 알려주겠니    O O O\n",
       "1  아니 먼지 정보 알려주세요  O O O O\n",
       "2    그 때 미세먼지 어떨까  O O O O\n",
       "3     그 때 먼지 좋으려나  O O O O\n",
       "4   미세먼지 어떨 것 같은데  O O O O"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dataset = pd.read_csv(dataset.entity_data_dir)\n",
    "\n",
    "entity_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf430a8-1c04-4074-ad6c-e2e246929b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE B-LOCATION E-LOCATION O O</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O B-LOCATION E-LOCATION O O O O</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O O</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-DATE E-DATE O O O</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT O S-LOCATION O O</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT O S-RESTAURANT O</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT S-LOCATION O</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT S-LOCATION O O O</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S-RESTAURANT S-RESTAURANT O O</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question\n",
       "label                                                  \n",
       "B-DATE E-DATE B-LOCATION E-LOCATION O O               4\n",
       "B-DATE E-DATE O                                       6\n",
       "B-DATE E-DATE O B-LOCATION E-LOCATION O O O O         1\n",
       "B-DATE E-DATE O O                                    42\n",
       "B-DATE E-DATE O O O                                  31\n",
       "...                                                 ...\n",
       "S-RESTAURANT O S-LOCATION O O                         4\n",
       "S-RESTAURANT O S-RESTAURANT O                         3\n",
       "S-RESTAURANT S-LOCATION O                             1\n",
       "S-RESTAURANT S-LOCATION O O O                         1\n",
       "S-RESTAURANT S-RESTAURANT O O                         2\n",
       "\n",
       "[354 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_dataset.groupby(['label']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e14b3-38b4-4784-ad95-b9364c93d1c3",
   "metadata": {},
   "source": [
    "# Bidirctional LSTM - CRF Models for sequence Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10025e55-144b-4bdb-a8d5-476b927dafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, w2v, tag_to_ix, hidden_dim, batch_size):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = w2v.size()[1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = w2v.size()[0]\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.batch_size = batch_size\n",
    "        self.START_TAG = \"<START_TAG>\"\n",
    "        self.STOP_TAG = \"<STOP_TAG>\"\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(self.vocab_size+2, self.embedding_dim)\n",
    "        self.word_embeds.weight[2:].data.copy_(w2v)\n",
    "        # self.word_embeds.weight.requires_grad = False\n",
    "        \n",
    "        # LSTM Parameter Define\n",
    "        # bidirectional : 2 way LSTM\n",
    "        # num_layers : layer count\n",
    "        # batch_first : pytorch LSTM input default value : (Length, batch, Hidden) line  then change the (batch, Length, Hidden)\n",
    "        # nn.LSTM(input_size, hidden_size, batch_first, num_layers)\n",
    "        # hidden_size = hidden_dim // reason the 2 because bidirectional = True\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim // 2 , batch_first = True, num_layers = 1, bidirectional = True )\n",
    "        \n",
    "        # LSTM output : Tag position \n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        # ouputing : rule train CRF setting\n",
    "        self.crf = CRF(self.tagset_size, batch_first = True)\n",
    "        \n",
    "    def init_hidden(self): #(h,c)\n",
    "        return (torch.randn(2, self.batch_size, self.hidden_dim // 2),\n",
    "                torch.randn(2, self.batch_size, self.hidden_dim // 2))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        # Bi-LSTM : output score get\n",
    "        self.batch_size = sentence.size()[0]\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        #(2, 128, 128), (2, 128, 128)\n",
    "        embeds = self.word_embeds(sentence)\n",
    "        \n",
    "        #(128, 20, 300)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        #(128, 20, 256), ((2, 128, 128), (2, 128, 128))\n",
    "        lstm_feats = self.hidden2tag(lstm_out) # (batch, length, tagset_size)\n",
    "        \n",
    "        #(128, 20, 17)\n",
    "        return lstm_feats\n",
    "    \n",
    "    def decode(self, logits, mask):\n",
    "        \"\"\"\n",
    "        Viterbi Decoding의 구현체입니다.\n",
    "        CRF 레이어의 출력을 prediction으로 변형합니다.\n",
    "        :param logits: 모델의 출력 (로짓)\n",
    "        :param mask: 마스킹 벡터\n",
    "        :return: 모델의 예측 (prediction)\n",
    "        \n",
    "        각 단어의 자리마다\n",
    "          word 1의 태그 확률        |  word2의 태그 확률\n",
    "         'O': 확률0,              | 'O': 확률A,\n",
    "         'B-DATE': 확률1,         | 'B-DATE': 확률B\n",
    "         'B-LOCATION': 확률2,     | 'B-LOCATION': 확률C,\n",
    "         'B-PLACE': 확률3,        | 'B-PLACE': 확률D,  \n",
    "         'B-RESTAURANT': 확률4,   | 'B-RESTAURANT': 확률E,  \n",
    "         'E-DATE': 확률5,         | 'E-DATE': 확률F,   \n",
    "         'E-LOCATION': 확률6,     | 'E-LOCATION': 확률G, \n",
    "         'E-PLACE': 확률7,        | 'E-PLACE': 확률H,  \n",
    "         'E-RESTAURANT': 확률8,   | 'E-RESTAURANT': 확률I, \n",
    "         'I-DATE': 확률9,         | 'I-DATE': 확률J,    \n",
    "         'I-RESTAURANT': 확률10,  | 'I-RESTAURANT': 확률K,\n",
    "         'S-DATE': 확률11,        | 'S-DATE': 확률L,      \n",
    "         'S-LOCATION': 확률12,    | 'S-LOCATION': 확률M,\n",
    "         'S-PLACE': 확률13,       | 'S-PLACE': 확률N,  \n",
    "         'S-RESTAURANT': 확률14,  | 'S-RESTAURANT': 확률O,\n",
    "         '<START_TAG>': 확률15,   | '<START_TAG>': 확률P, \n",
    "         '<STOP_TAG>': 확률15,    | '<STOP_TAG>': 확률Q,\n",
    "         \n",
    "         각각의 높은 확률을 뽑는 것은 보통의 딥러닝 방식으로 B단독이나 I단독, E단독같은 문제를 야기할 수 있습니다.\n",
    "         태그들의 확률 값을 받아서 \n",
    "         CRF는 태그들의 의존성을 학습할수 있어서 태그 시퀀스의 확률이 가장 높은 확률을 가지는 예측 시퀀스를 선택한다.\n",
    "         그래서 B단독이나 I단독, E단독과 같은 문제를 없애줍니다.\n",
    "         예를 들어 B-DATE, O 와 같은걸 출력하지 않습니다. (CRF는 S-DATE, O 라고 출력합니다.)\n",
    "        \"\"\"\n",
    "        return self.crf.decode(logits, mask)\n",
    "    \n",
    "    def compute_loss(self, label, logits, mask):\n",
    "        \"\"\"\n",
    "        학습을 위한 total loss 계산\n",
    "        :param label : label\n",
    "        :param logits : logits\n",
    "        :param mask : mask vector\n",
    "        :return : total loss\n",
    "        \"\"\"\n",
    "        \n",
    "        log_likelihood = self.crf(logits, label, mask = mask, reduction='mean')\n",
    "        return - log_likelihood  # Negative log likelihood loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bae5f1a-664d-4795-9415-b48f47aaa568",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = MakeEmbed()\n",
    "embed.load_word2vec()\n",
    "\n",
    "entity_train_dataset, entity_test_dataset = dataset.make_entity_dataset(embed)\n",
    "\n",
    "train_dataloader = DataLoader(entity_train_dataset, batch_size = 128, shuffle = True)\n",
    "\n",
    "test_dataloader = DataLoader(entity_test_dataset, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0adc7b71-9fc3-4688-a256-6653c1c525a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75,   9, 139,  ...,   0,   0,   0],\n",
       "        [405, 133,   0,  ...,   0,   0,   0],\n",
       "        [428,   2,  56,  ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [104, 303, 475,  ...,   0,   0,   0],\n",
       "        [278, 255,  37,  ...,   0,   0,   0],\n",
       "        [222, 210,  31,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_train_dataset.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ce232ad-7107-495b-9bb8-03bf419bf40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  0, 13,  ...,  0,  0,  0],\n",
       "        [12,  0,  0,  ...,  0,  0,  0],\n",
       "        [12,  0,  0,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0, 12, 14,  ...,  0,  0,  0],\n",
       "        [11,  0,  0,  ...,  0,  0,  0],\n",
       "        [12,  4,  8,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_train_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e535e321-7b57-4d4d-97b2-2b000467a002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(1481, 300)\n",
       "  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=256, out_features=17, bias=True)\n",
       "  (crf): CRF(num_tags=17)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = embed.word2vec.wv.vectors\n",
    "weights = torch.FloatTensor(weights)\n",
    "\n",
    "model = BiLSTM_CRF(weights, dataset.entity_label, 256, 128)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9e4a1-8f85-419e-b150-7fae294b9dcd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42df416e-8e67-45e1-aa4b-095d9cceae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|████████████████████████████████████████████████████████| 125/125 [00:22<00:00,  5.54batch/s, loss=0.462]\n",
      "Epoch 0: 100%|████████████████████████████████████████████| 32/32 [00:02<00:00, 12.02batch/s, accuracy=20, loss=0.0908]\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████| 125/125 [00:23<00:00,  5.29batch/s, loss=0.419]\n",
      "Epoch 1: 100%|█████████████████████████████████████████| 32/32 [00:02<00:00, 10.94batch/s, accuracy=20.5, loss=0.00626]\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████| 125/125 [00:24<00:00,  5.05batch/s, loss=0.0576]\n",
      "Epoch 2: 100%|███████████████████████████████████████████| 32/32 [00:02<00:00, 11.13batch/s, accuracy=20.9, loss=0.122]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████| 125/125 [00:25<00:00,  4.91batch/s, loss=0.102]\n",
      "Epoch 3: 100%|███████████████████████████████████████████| 32/32 [00:03<00:00,  9.45batch/s, accuracy=20.7, loss=0.082]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████| 125/125 [00:26<00:00,  4.77batch/s, loss=0.138]\n",
      "Epoch 4: 100%|█████████████████████████████████████████| 32/32 [00:03<00:00,  9.36batch/s, accuracy=20.3, loss=0.00385]\n"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "prev_acc = 0\n",
    "save_dir = './chatbot_data/pretraining/1_entity_recog_model'\n",
    "save_prefix = 'entity_recog'\n",
    "for i in range(epoch):\n",
    "    steps = 0\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            y = data[1]\n",
    "            length = data[2]\n",
    "            \n",
    "            logits = model.forward(x)\n",
    "            # padding 된 부분을 마스킹하기위한 코드\n",
    "            # 우리는 length값이 존재하여 length값을 이용해서 마스크를 생성해도 가능\n",
    "            # 하지만 코드 간략화를 위해 pytorch에 where 함수를 이용해 마스크 생성\n",
    "            # torch.where 함수 설명 : https://runebook.dev/ko/docs/pytorch/generated/torch.where\n",
    "            mask = torch.where(x > 0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n",
    "            loss = model.compute_loss(y, logits, mask)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    steps = 0\n",
    "    accuracy_list = []\n",
    "    \n",
    "    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            y = data[1]\n",
    "            length = data[2]\n",
    "            \n",
    "            mask = torch.where(x > 0, torch.tensor([1.]), torch.tensor([0.1])).type(torch.uint8)\n",
    "            logits = model.forward(x)\n",
    "            \n",
    "            predicts = model.decode(logits, mask)\n",
    "            \n",
    "            corrects = []\n",
    "            for target, leng, predict in zip(y, length, predicts):\n",
    "                corrects.append(target[:leng].tolist() == predict)\n",
    "                \n",
    "            accuracy = 100.0 * sum(corrects) / len(corrects)\n",
    "            accuracy_list.append(accuracy)\n",
    "            \n",
    "            loss = model.compute_loss(y, logits, mask)\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy = sum(accuracy_list)/len(accuracy_list))\n",
    "            \n",
    "        acc = sum(accuracy_list)/len(accuracy_list)\n",
    "        if(acc>prev_acc):\n",
    "            prev_acc = acc\n",
    "            save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)),i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517701b1-534b-4274-8a54-06fc366d30d6",
   "metadata": {},
   "source": [
    "# Load & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4df26e99-69d2-48bf-b2af-75f3f1315b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CRF(\n",
       "  (word_embeds): Embedding(1481, 300)\n",
       "  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=256, out_features=17, bias=True)\n",
       "  (crf): CRF(num_tags=17)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./chatbot_data/pretraining/1_entity_recog_model/entity_recog_20.89_steps_2.pt'))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ed0642d-3138-48ee-b2ff-fcbd3cc48382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 : 이번 , 태그 : B-DATE\n",
      "단어 : 주 , 태그 : E-DATE\n",
      "단어 : 날씨 , 태그 : O\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 6.97 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = '이번 주 날씨'\n",
    "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
    "\n",
    "x = torch.tensor(x)\n",
    "f = model(x.unsqueeze(0))\n",
    "\n",
    "mask = torch.where(x>0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n",
    "\n",
    "predict = model.decode(f, mask.view(1, -1))\n",
    "\n",
    "# s : SOLO\n",
    "# B : duplicate start\n",
    "# I : duplicate middle\n",
    "# E : duplicate end\n",
    "\n",
    "tag = [dataset.entitys[p] for p in predict[0]]\n",
    "for i, j in zip(q.split(' '), tag):\n",
    "    print('단어 : '+i+\" , \"+\"태그 : \"+j )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "288a6d88-8444-4fad-a0fe-48532a59f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 : 나 , 태그 : O\n",
      "단어 : 내일 , 태그 : S-DATE\n",
      "단어 : 제주도 , 태그 : S-LOCATION\n",
      "단어 : 여행 , 태그 : O\n",
      "단어 : 가는데 , 태그 : O\n",
      "단어 : 미세먼지 , 태그 : O\n",
      "단어 : 알려줘 , 태그 : O\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 5.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = '나 내일 제주도 여행 가는데 미세먼지 알려줘'\n",
    "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
    "\n",
    "x = torch.tensor(x)\n",
    "f = model(x.unsqueeze(0))\n",
    "\n",
    "mask = torch.where(x>0, torch.tensor([1.]), torch.tensor([0.])).type(torch.uint8)\n",
    "\n",
    "predict = model.decode(f, mask.view(1, -1))\n",
    "\n",
    "# s : SOLO\n",
    "# B : duplicate start\n",
    "# I : duplicate middle\n",
    "# E : duplicate end\n",
    "\n",
    "tag = [dataset.entitys[p] for p in predict[0]]\n",
    "for i, j in zip(q.split(' '), tag):\n",
    "    print('단어 : '+i+\" , \"+\"태그 : \"+j )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
